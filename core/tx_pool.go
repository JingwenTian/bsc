// Copyright 2014 The go-ethereum Authors
// This file is part of the go-ethereum library.
//
// The go-ethereum library is free software: you can redistribute it and/or modify
// it under the terms of the GNU Lesser General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// The go-ethereum library is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
// GNU Lesser General Public License for more details.
//
// You should have received a copy of the GNU Lesser General Public License
// along with the go-ethereum library. If not, see <http://www.gnu.org/licenses/>.

package core

import (
	"container/heap"
	"errors"
	"math"
	"math/big"
	"sort"
	"sync"
	"sync/atomic"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/prque"
	"github.com/ethereum/go-ethereum/consensus/misc"
	"github.com/ethereum/go-ethereum/core/state"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/event"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/metrics"
	"github.com/ethereum/go-ethereum/params"
)

const (
	// chainHeadChanSize is the size of channel listening to ChainHeadEvent.
	chainHeadChanSize = 9

	// txSlotSize is used to calculate how many data slots a single transaction
	// takes up based on its size. The slots are used as DoS protection, ensuring
	// that validating a new transaction remains a constant operation (in reality
	// O(maxslots), where max slots are 4 currently).
	txSlotSize = 32 * 1024

	// txMaxSize is the maximum size a single transaction can have. This field has
	// non-trivial consequences: larger transactions are significantly harder and
	// more expensive to propagate; larger transactions also take more resources
	// to validate whether they fit into the pool or not.
	txMaxSize = 4 * txSlotSize // 128KB

	// txReannoMaxNum is the maximum number of transactions a reannounce action can include.
	txReannoMaxNum = 1024
)

var (
	// ErrAlreadyKnown is returned if the transactions is already contained
	// within the pool.
	ErrAlreadyKnown = errors.New("already known")

	// ErrInvalidSender is returned if the transaction contains an invalid signature.
	ErrInvalidSender = errors.New("invalid sender")

	// ErrUnderpriced is returned if a transaction's gas price is below the minimum
	// configured for the transaction pool.
	ErrUnderpriced = errors.New("transaction underpriced")

	// ErrTxPoolOverflow is returned if the transaction pool is full and can't accpet
	// another remote transaction.
	ErrTxPoolOverflow = errors.New("txpool is full")

	// ErrReplaceUnderpriced is returned if a transaction is attempted to be replaced
	// with a different one without the required price bump.
	ErrReplaceUnderpriced = errors.New("replacement transaction underpriced")

	// ErrGasLimit is returned if a transaction's requested gas limit exceeds the
	// maximum allowance of the current block.
	ErrGasLimit = errors.New("exceeds block gas limit")

	// ErrNegativeValue is a sanity error to ensure no one is able to specify a
	// transaction with a negative value.
	ErrNegativeValue = errors.New("negative value")

	// ErrOversizedData is returned if the input data of a transaction is greater
	// than some meaningful limit a user might use. This is not a consensus error
	// making the transaction invalid, rather a DOS protection.
	ErrOversizedData = errors.New("oversized data")

	// ErrFutureReplacePending is returned if a future transaction replaces a pending
	// transaction. Future transactions should only be able to replace other future transactions.
	ErrFutureReplacePending = errors.New("future transaction tries to replace pending")

	// ErrOverdraft is returned if a transaction would cause the senders balance to go negative
	// thus invalidating a potential large number of transactions.
	ErrOverdraft = errors.New("transaction would cause overdraft")
)

var (
	evictionInterval    = time.Minute     // Time interval to check for evictable transactions
	statsReportInterval = 8 * time.Second // Time interval to report transaction pool stats
	reannounceInterval  = time.Minute     // Time interval to check for reannounce transactions
)

var (
	// Metrics for the pending pool
	pendingDiscardMeter   = metrics.NewRegisteredMeter("txpool/pending/discard", nil)
	pendingReplaceMeter   = metrics.NewRegisteredMeter("txpool/pending/replace", nil)
	pendingRateLimitMeter = metrics.NewRegisteredMeter("txpool/pending/ratelimit", nil) // Dropped due to rate limiting
	pendingNofundsMeter   = metrics.NewRegisteredMeter("txpool/pending/nofunds", nil)   // Dropped due to out-of-funds

	// Metrics for the queued pool
	queuedDiscardMeter   = metrics.NewRegisteredMeter("txpool/queued/discard", nil)
	queuedReplaceMeter   = metrics.NewRegisteredMeter("txpool/queued/replace", nil)
	queuedRateLimitMeter = metrics.NewRegisteredMeter("txpool/queued/ratelimit", nil) // Dropped due to rate limiting
	queuedNofundsMeter   = metrics.NewRegisteredMeter("txpool/queued/nofunds", nil)   // Dropped due to out-of-funds
	queuedEvictionMeter  = metrics.NewRegisteredMeter("txpool/queued/eviction", nil)  // Dropped due to lifetime

	// General tx metrics
	knownTxMeter       = metrics.NewRegisteredMeter("txpool/known", nil)
	validTxMeter       = metrics.NewRegisteredMeter("txpool/valid", nil)
	invalidTxMeter     = metrics.NewRegisteredMeter("txpool/invalid", nil)
	underpricedTxMeter = metrics.NewRegisteredMeter("txpool/underpriced", nil)
	overflowedTxMeter  = metrics.NewRegisteredMeter("txpool/overflowed", nil)
	// throttleTxMeter counts how many transactions are rejected due to too-many-changes between
	// txpool reorgs.
	throttleTxMeter = metrics.NewRegisteredMeter("txpool/throttle", nil)
	// reorgDurationTimer measures how long time a txpool reorg takes.
	reorgDurationTimer = metrics.NewRegisteredTimer("txpool/reorgtime", nil)
	// dropBetweenReorgHistogram counts how many drops we experience between two reorg runs. It is expected
	// that this number is pretty low, since txpool reorgs happen very frequently.
	dropBetweenReorgHistogram = metrics.NewRegisteredHistogram("txpool/dropbetweenreorg", nil, metrics.NewExpDecaySample(1028, 0.015))

	pendingGauge = metrics.NewRegisteredGauge("txpool/pending", nil)
	queuedGauge  = metrics.NewRegisteredGauge("txpool/queued", nil)
	localGauge   = metrics.NewRegisteredGauge("txpool/local", nil)
	slotsGauge   = metrics.NewRegisteredGauge("txpool/slots", nil)

	reheapTimer = metrics.NewRegisteredTimer("txpool/reheap", nil)
)

// TxStatus is the current status of a transaction as seen by the pool.
type TxStatus uint

const (
	TxStatusUnknown TxStatus = iota
	TxStatusQueued
	TxStatusPending
	TxStatusIncluded
)

// blockChain provides the state of blockchain and current gas limit to do
// some pre checks in tx pool and event subscribers.
type blockChain interface {
	CurrentBlock() *types.Block
	GetBlock(hash common.Hash, number uint64) *types.Block
	StateAt(root common.Hash) (*state.StateDB, error)

	SubscribeChainHeadEvent(ch chan<- ChainHeadEvent) event.Subscription
}

// TxPoolConfig are the configuration parameters of the transaction pool.
// äº¤æ˜“æ± é…ç½®
// 1. ä¸¤ä¸ªé‡è¦æ¦‚å¿µã€Œå¯æ‰§è¡Œäº¤æ˜“ã€å’Œã€Œéå¯æ‰§è¡Œäº¤æ˜“ã€ã€‚
//   - å¯æ‰§è¡Œäº¤æ˜“æ˜¯æŒ‡ä»äº¤æ˜“æ± ä¸­æ‹©ä¼˜é€‰å‡ºçš„ä¸€éƒ¨åˆ†äº¤æ˜“å¯ä»¥è¢«æ‰§è¡Œï¼Œæ‰“åŒ…åˆ°åŒºå—ä¸­ã€‚
//   - éå¯æ‰§è¡Œäº¤æ˜“åˆ™ç›¸åï¼Œä»»ä½•åˆšè¿›å…¥äº¤æ˜“æ± çš„äº¤æ˜“å‡å±äºéå¯æ‰§è¡ŒçŠ¶æ€ï¼Œåœ¨æŸä¸€ä¸ªæ—¶åˆ»æ‰ä¼šæå‡ä¸ºå¯æ‰§è¡ŒçŠ¶æ€ã€‚
//
// 2. åœ¨äº¤æ˜“æ± ä¸­å°†äº¤æ˜“æ ‡è®°ä¸º local çš„æœ‰å¤šç§ç”¨é€”ï¼š
//   - åœ¨æœ¬åœ°ç£ç›˜å­˜å‚¨å·²å‘é€çš„äº¤æ˜“ã€‚è¿™æ ·ï¼Œæœ¬åœ°äº¤æ˜“ä¸ä¼šä¸¢å¤±ï¼Œé‡å¯èŠ‚ç‚¹æ—¶å¯ä»¥é‡æ–°åŠ è½½åˆ°äº¤æ˜“æ± ï¼Œå®æ—¶å¹¿æ’­å‡ºå»ã€‚
//   - å¯ä»¥ä½œä¸ºå¤–éƒ¨ç¨‹åºå’Œä»¥å¤ªåŠæ²Ÿé€šçš„ä¸€ä¸ªæ¸ é“ã€‚å¤–éƒ¨ç¨‹åºåªéœ€è¦ç›‘å¬æ–‡ä»¶å†…å®¹å˜åŒ–ï¼Œåˆ™å¯ä»¥è·å¾—äº¤æ˜“æ¸…å•ã€‚
//   - localäº¤æ˜“å¯ä¼˜å…ˆäº remote äº¤æ˜“ã€‚å¯¹äº¤æ˜“é‡çš„é™åˆ¶ç­‰æ“ä½œï¼Œä¸å½±å“ local ä¸‹çš„è´¦æˆ·å’Œäº¤æ˜“ã€‚
//
// ä»¥å¤ªåŠ geth èŠ‚ç‚¹å…è®¸åœ¨å¯åŠ¨èŠ‚ç‚¹æ—¶ï¼Œé€šè¿‡å‚æ•°ä¿®æ”¹é…ç½® (é€šè¿‡ geth -h æŸ¥çœ‹ --txpool.*çš„ä¸€ç³»åˆ—é…ç½®)
type TxPoolConfig struct {
	Locals    []common.Address // å®šä¹‰äº†ä¸€ç»„è§†ä¸ºlocaläº¤æ˜“çš„è´¦æˆ·åœ°å€ã€‚ä»»ä½•æ¥è‡ªæ­¤æ¸…å•çš„äº¤æ˜“å‡è¢«è§†ä¸º local äº¤æ˜“ã€‚
	NoLocals  bool             // æ˜¯å¦ç¦æ­¢localäº¤æ˜“å¤„ç†ã€‚é»˜è®¤ä¸º fasle,å…è®¸ local äº¤æ˜“ã€‚å¦‚æœç¦æ­¢ï¼Œåˆ™æ¥è‡ª local çš„äº¤æ˜“å‡è§†ä¸º remote äº¤æ˜“å¤„ç†ã€‚
	Journal   string           // å­˜å‚¨localäº¤æ˜“è®°å½•çš„æ–‡ä»¶åï¼Œé»˜è®¤æ˜¯ ./transactions.rlpã€‚
	Rejournal time.Duration    //å®šæœŸå°†localäº¤æ˜“å­˜å‚¨æ–‡ä»¶ä¸­çš„æ—¶é—´é—´éš”ã€‚é»˜è®¤ä¸ºæ¯å°æ—¶ä¸€æ¬¡ã€‚

	PriceLimit uint64 // remoteäº¤æ˜“è¿›å…¥äº¤æ˜“æ± çš„æœ€ä½ Price è¦æ±‚ã€‚æ­¤è®¾ç½®å¯¹ local äº¤æ˜“æ— æ•ˆã€‚é»˜è®¤å€¼1ã€‚
	PriceBump  uint64 // æ›¿æ¢äº¤æ˜“æ—¶æ‰€è¦æ±‚çš„ä»·æ ¼ä¸Šè°ƒæ¶¨å¹…æ¯”ä¾‹æœ€ä½è¦æ±‚ã€‚ä»»ä½•ä½äºè¦æ±‚çš„æ›¿æ¢äº¤æ˜“å‡è¢«æ‹’ç»ã€‚

	AccountSlots uint64 // å½“äº¤æ˜“æ± ä¸­å¯æ‰§è¡Œäº¤æ˜“ï¼ˆæ˜¯å·²åœ¨ç­‰å¾…çŸ¿å·¥æ‰“åŒ…çš„äº¤æ˜“ï¼‰é‡è¶…æ ‡æ—¶ï¼Œå…è®¸æ¯ä¸ªè´¦æˆ·å¯ä»¥ä¿ç•™åœ¨äº¤æ˜“æ± æœ€ä½äº¤æ˜“æ•°ã€‚é»˜è®¤å€¼æ˜¯ 16 ç¬”ã€‚
	GlobalSlots  uint64 // äº¤æ˜“æ± ä¸­æ‰€å…è®¸çš„å¯æ‰§è¡Œäº¤æ˜“é‡ä¸Šé™ï¼Œé«˜äºä¸Šé™æ—¶å°†é‡Šæ”¾éƒ¨åˆ†äº¤æ˜“ã€‚é»˜è®¤æ˜¯ 4096 ç¬”äº¤æ˜“ã€‚
	AccountQueue uint64 // äº¤æ˜“æ± ä¸­å•ä¸ªè´¦æˆ·éå¯æ‰§è¡Œäº¤æ˜“ä¸Šé™ï¼Œé»˜è®¤æ˜¯64ç¬”ã€‚
	GlobalQueue  uint64 // äº¤æ˜“æ± ä¸­æ‰€æœ‰éå¯æ‰§è¡Œäº¤æ˜“ä¸Šé™ï¼Œé»˜è®¤1024 ç¬”ã€‚

	Lifetime       time.Duration // å…è®¸ remote çš„éå¯æ‰§è¡Œäº¤æ˜“å¯åœ¨äº¤æ˜“æ± å­˜æ´»çš„æœ€é•¿æ—¶é—´ã€‚äº¤æ˜“æ± æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼Œä¸€æ—¦å‘ç°æœ‰è¶…æœŸçš„remote è´¦æˆ·ï¼Œåˆ™ç§»é™¤è¯¥è´¦æˆ·ä¸‹çš„æ‰€æœ‰éå¯æ‰§è¡Œäº¤æ˜“ã€‚é»˜è®¤ä¸º3å°æ—¶ã€‚
	ReannounceTime time.Duration // Duration for announcing local pending transactions again
}

// DefaultTxPoolConfig contains the default configurations for the transaction
// pool.
var DefaultTxPoolConfig = TxPoolConfig{
	Journal:   "transactions.rlp",
	Rejournal: time.Hour,

	PriceLimit: 1,
	PriceBump:  10,

	AccountSlots: 16,
	GlobalSlots:  4096 + 1024, // urgent + floating queue capacity with 4:1 ratio
	AccountQueue: 64,
	GlobalQueue:  1024,

	Lifetime:       3 * time.Hour,
	ReannounceTime: 10 * 365 * 24 * time.Hour,
}

// sanitize checks the provided user configurations and changes anything that's
// unreasonable or unworkable.
func (config *TxPoolConfig) sanitize() TxPoolConfig {
	conf := *config
	if conf.Rejournal < time.Second {
		log.Warn("Sanitizing invalid txpool journal time", "provided", conf.Rejournal, "updated", time.Second)
		conf.Rejournal = time.Second
	}
	if conf.PriceLimit < 1 {
		log.Warn("Sanitizing invalid txpool price limit", "provided", conf.PriceLimit, "updated", DefaultTxPoolConfig.PriceLimit)
		conf.PriceLimit = DefaultTxPoolConfig.PriceLimit
	}
	if conf.PriceBump < 1 {
		log.Warn("Sanitizing invalid txpool price bump", "provided", conf.PriceBump, "updated", DefaultTxPoolConfig.PriceBump)
		conf.PriceBump = DefaultTxPoolConfig.PriceBump
	}
	if conf.AccountSlots < 1 {
		log.Warn("Sanitizing invalid txpool account slots", "provided", conf.AccountSlots, "updated", DefaultTxPoolConfig.AccountSlots)
		conf.AccountSlots = DefaultTxPoolConfig.AccountSlots
	}
	if conf.GlobalSlots < 1 {
		log.Warn("Sanitizing invalid txpool global slots", "provided", conf.GlobalSlots, "updated", DefaultTxPoolConfig.GlobalSlots)
		conf.GlobalSlots = DefaultTxPoolConfig.GlobalSlots
	}
	if conf.AccountQueue < 1 {
		log.Warn("Sanitizing invalid txpool account queue", "provided", conf.AccountQueue, "updated", DefaultTxPoolConfig.AccountQueue)
		conf.AccountQueue = DefaultTxPoolConfig.AccountQueue
	}
	if conf.GlobalQueue < 1 {
		log.Warn("Sanitizing invalid txpool global queue", "provided", conf.GlobalQueue, "updated", DefaultTxPoolConfig.GlobalQueue)
		conf.GlobalQueue = DefaultTxPoolConfig.GlobalQueue
	}
	if conf.Lifetime < 1 {
		log.Warn("Sanitizing invalid txpool lifetime", "provided", conf.Lifetime, "updated", DefaultTxPoolConfig.Lifetime)
		conf.Lifetime = DefaultTxPoolConfig.Lifetime
	}
	if conf.ReannounceTime < time.Minute {
		log.Warn("Sanitizing invalid txpool reannounce time", "provided", conf.ReannounceTime, "updated", time.Minute)
		conf.ReannounceTime = time.Minute
	}
	return conf
}

// TxPool contains all currently known transactions. Transactions
// enter the pool when they are received from the network or submitted
// locally. They exit the pool when they are included in the blockchain.
//
// The pool separates processable transactions (which can be applied to the
// current state) and future transactions. Transactions move between those
// two states over time as they are received and processed.
type TxPool struct {
	config       TxPoolConfig
	chainconfig  *params.ChainConfig
	chain        blockChain
	gasPrice     *big.Int
	txFeed       event.Feed
	reannoTxFeed event.Feed // Event feed for announcing transactions again
	scope        event.SubscriptionScope
	signer       types.Signer
	mu           sync.RWMutex

	istanbul bool // Fork indicator whether we are in the istanbul stage.
	eip2718  bool // Fork indicator whether we are using EIP-2718 type transactions.
	eip1559  bool // Fork indicator whether we are using EIP-1559 type transactions.

	currentState  *state.StateDB // Current state in the blockchain head
	pendingNonces *txNoncer      // Pending state tracking virtual nonces
	currentMaxGas uint64         // Current gas limit for transaction caps

	locals  *accountSet // Set of local transaction to exempt from eviction rules
	journal *txJournal  // Journal of local transaction to back up to disk

	pending map[common.Address]*txList   // All currently processable transactions
	queue   map[common.Address]*txList   // Queued but non-processable transactions
	beats   map[common.Address]time.Time // Last heartbeat from each known account
	all     *txLookup                    // All transactions to allow lookups
	priced  *txPricedList                // All transactions sorted by price

	chainHeadCh     chan ChainHeadEvent
	chainHeadSub    event.Subscription
	reqResetCh      chan *txpoolResetRequest
	reqPromoteCh    chan *accountSet
	queueTxEventCh  chan *types.Transaction
	reorgDoneCh     chan chan struct{}
	reorgShutdownCh chan struct{}  // requests shutdown of scheduleReorgLoop
	wg              sync.WaitGroup // tracks loop, scheduleReorgLoop
	initDoneCh      chan struct{}  // is closed once the pool is initialized (for tests)

	changesSinceReorg int // A counter for how many drops we've performed in-between reorg.
}

type txpoolResetRequest struct {
	oldHead, newHead *types.Header
}

// NewTxPool creates a new transaction pool to gather, sort and filter inbound
// transactions from the network.
// åˆ›å»ºä¸€ä¸ªæ–°çš„äº¤æ˜“æ± ï¼Œç”¨äºæ”¶é›†ã€æ’åºå’Œè¿‡æ»¤æ¥è‡ªç½‘ç»œçš„ä¼ å…¥äº¤æ˜“ã€‚
func NewTxPool(config TxPoolConfig, chainconfig *params.ChainConfig, chain blockChain) *TxPool {
	// Sanitize the input to ensure no vulnerable gas prices are set
	// å¯¹è¾“å…¥çš„é…ç½®è¿›è¡Œæ¸…ç†ï¼Œä»¥ç¡®ä¿æ²¡æœ‰è®¾ç½®ä¸å®‰å…¨çš„Gasä»·æ ¼ã€‚
	config = (&config).sanitize()

	// Create the transaction pool with its initial settings
	// ä½¿ç”¨åˆå§‹è®¾ç½®åˆ›å»ºäº¤æ˜“æ± 
	pool := &TxPool{
		config:          config,                                       // ä¼ å…¥çš„äº¤æ˜“æ± é…ç½®
		chainconfig:     chainconfig,                                  // ä¼ å…¥çš„é“¾é…ç½®
		chain:           chain,                                        // ä¼ å…¥çš„åŒºå—é“¾å¯¹è±¡
		signer:          types.LatestSigner(chainconfig),              // æ ¹æ®é“¾é…ç½®åˆ›å»ºçš„ç­¾åå™¨
		pending:         make(map[common.Address]*txList),             // ç”¨äºå­˜å‚¨å¾…å¤„ç†çš„äº¤æ˜“åˆ—è¡¨
		queue:           make(map[common.Address]*txList),             // ç”¨äºå­˜å‚¨æ’é˜Ÿçš„äº¤æ˜“åˆ—è¡¨
		beats:           make(map[common.Address]time.Time),           // ç”¨äºå­˜å‚¨äº¤æ˜“çš„å¿ƒè·³æ—¶é—´
		all:             newTxLookup(),                                // äº¤æ˜“æŸ¥æ‰¾è¡¨
		chainHeadCh:     make(chan ChainHeadEvent, chainHeadChanSize), // ç”¨äºæ¥æ”¶é“¾å¤´äº‹ä»¶çš„é€šé“
		reqResetCh:      make(chan *txpoolResetRequest),               // ç”¨äºæ¥æ”¶é‡ç½®äº¤æ˜“æ± è¯·æ±‚çš„é€šé“
		reqPromoteCh:    make(chan *accountSet),                       // ç”¨äºæ¥æ”¶æå‡è´¦æˆ·ä¼˜å…ˆçº§è¯·æ±‚çš„é€šé“
		queueTxEventCh:  make(chan *types.Transaction),                // ç”¨äºæ¥æ”¶æ’é˜Ÿäº¤æ˜“äº‹ä»¶çš„é€šé“
		reorgDoneCh:     make(chan chan struct{}),                     // ç”¨äºæ¥æ”¶é‡ç»„å®Œæˆä¿¡å·çš„é€šé“
		reorgShutdownCh: make(chan struct{}),                          // ç”¨äºæ¥æ”¶é‡ç»„å…³é—­ä¿¡å·çš„é€šé“
		initDoneCh:      make(chan struct{}),                          // ç”¨äºæ¥æ”¶åˆå§‹åŒ–å®Œæˆä¿¡å·çš„é€šé“
		gasPrice:        new(big.Int).SetUint64(config.PriceLimit),    // Gasä»·æ ¼é™åˆ¶
	}
	// åˆ›å»ºæœ¬åœ°è´¦æˆ·é›†åˆï¼Œå¹¶å°†é…ç½®ä¸­çš„æœ¬åœ°è´¦æˆ·æ·»åŠ åˆ°é›†åˆä¸­ã€‚
	// ğŸŒŸ å¯ä»¥è®¾ç½®ä¸€ç»„è§†ä¸ºlocaläº¤æ˜“çš„è´¦æˆ·åœ°å€, è¿™äº›åœ°å€çš„äº¤æ˜“å‡è¢«è§†ä¸º local äº¤æ˜“, å½“å­˜åœ¨å¤šä¸ªèŠ‚ç‚¹æ—¶å¯ä»¥è®¾ç½®ä»¥æé«˜äº¤æ˜“ä¼˜å…ˆçº§
	pool.locals = newAccountSet(pool.signer)
	for _, addr := range config.Locals {
		log.Info("Setting new local account", "address", addr)
		pool.locals.add(addr)
	}
	// åˆ›å»ºæŒ‰ç…§ä»·æ ¼æ’åºçš„äº¤æ˜“åˆ—è¡¨
	pool.priced = newTxPricedList(pool.all)
	// è°ƒç”¨ reset æ–¹æ³•ï¼Œé‡ç½®äº¤æ˜“æ± ï¼Œå¹¶ä½¿ç”¨å½“å‰åŒºå—å¤´åˆå§‹åŒ–ã€‚
	pool.reset(nil, chain.CurrentBlock().Header())

	// Start the reorg loop early so it can handle requests generated during journal loading.
	// å¯åŠ¨é‡ç»„å¾ªç¯ï¼Œä»¥ä¾¿åœ¨æ—¥å¿—åŠ è½½æœŸé—´å¤„ç†ç”Ÿæˆçš„è¯·æ±‚ã€‚
	pool.wg.Add(1)
	go pool.scheduleReorgLoop()

	// If local transactions and journaling is enabled, load from disk
	// å¯¹åº”æœ¬åœ°äº¤æ˜“å­˜å‚¨ï¼Œåœ¨å¯åŠ¨äº¤æ˜“æ± æ—¶æ ¹æ®é…ç½®å¼€å¯æœ¬åœ°äº¤æ˜“å­˜å‚¨èƒ½åŠ›ï¼š
	if !config.NoLocals && config.Journal != "" {
		pool.journal = newTxJournal(config.Journal)
		// ä»ç£ç›˜ä¸­åŠ è½½å·²æœ‰äº¤æ˜“åˆ°äº¤æ˜“æ± 
		if err := pool.journal.load(pool.AddLocals); err != nil {
			log.Warn("Failed to load transaction journal", "err", err)
		}
		// å°†äº¤æ˜“æ± ä¸­çš„æ‰€æœ‰æœ¬åœ°äº¤æ˜“è¦†ç›–journalæ–‡ä»¶
		if err := pool.journal.rotate(pool.local()); err != nil {
			log.Warn("Failed to rotate transaction journal", "err", err)
		}
	}

	// åœ¨äº¤æ˜“æ± å¯åŠ¨åï¼Œå°†è®¢é˜…é“¾çš„åŒºå—å¤´äº‹ä»¶
	// æ‰€æœ‰è¿›å…¥äº¤æ˜“æ± çš„äº¤æ˜“å‡éœ€è¦è¢«æ ¡éªŒï¼Œæœ€åŸºæœ¬çš„æ˜¯æ ¡éªŒè´¦æˆ·ä½™é¢æ˜¯å¦è¶³å¤Ÿæ”¯ä»˜äº¤æ˜“æ‰§è¡Œã€‚æˆ–è€… äº¤æ˜“ nonce æ˜¯å¦åˆæ³•ã€‚
	// åœ¨äº¤æ˜“æ± ä¸­ç»´æŠ¤çš„æœ€æ–°çš„åŒºå—StateDBã€‚å½“äº¤æ˜“æ± æ¥æ”¶åˆ°æ–°åŒºå—ä¿¡å·æ—¶ï¼Œå°†ç«‹å³é‡ç½® statedbã€‚
	// Subscribe events from blockchain and start the main event loop.
	pool.chainHeadSub = pool.chain.SubscribeChainHeadEvent(pool.chainHeadCh)
	// å¹¶å¯åŠ¨ä¸»äº‹ä»¶å¾ªç¯
	pool.wg.Add(1)
	go pool.loop()

	return pool
}

// loop is the transaction pool's main event loop, waiting for and reacting to
// outside blockchain events as well as for various reporting and transaction
// eviction events.
func (pool *TxPool) loop() {
	defer pool.wg.Done()

	var (
		prevPending, prevQueued, prevStales int
		// Start the stats reporting and transaction eviction tickers
		report     = time.NewTicker(statsReportInterval)
		evict      = time.NewTicker(evictionInterval)
		reannounce = time.NewTicker(reannounceInterval)
		journal    = time.NewTicker(pool.config.Rejournal) // journalæœ¬åœ°äº¤æ˜“å­˜å‚¨è®¡æ—¶å™¨
		// Track the previous head headers for transaction reorgs
		head = pool.chain.CurrentBlock()
	)
	defer report.Stop()
	defer evict.Stop()
	defer reannounce.Stop()
	defer journal.Stop()

	// Notify tests that the init phase is done
	close(pool.initDoneCh)
	for {
		select {
		// Handle ChainHeadEvent
		// å¼€å§‹ç›‘å¬æ–°åŒºå—å¤´äº‹ä»¶
		case ev := <-pool.chainHeadCh:
			if ev.Block != nil {
				// æ¥æ”¶åˆ°äº‹ä»¶åï¼Œæ›´æ–° stateå’Œå¤„ç†äº¤æ˜“.
				// æ ¸å¿ƒæ˜¯å°†äº¤æ˜“æ± ä¸­å·²ç»ä¸ç¬¦åˆè¦æ±‚çš„äº¤æ˜“åˆ é™¤å¹¶æ›´æ–°æ•´ç†äº¤æ˜“.
				pool.requestReset(head.Header(), ev.Block.Header())
				head = ev.Block
			}

		// System shutdown.
		case <-pool.chainHeadSub.Err():
			close(pool.reorgShutdownCh)
			return

		// Handle stats reporting ticks
		case <-report.C:
			pool.mu.RLock()
			pending, queued := pool.stats()
			pool.mu.RUnlock()
			stales := int(atomic.LoadInt64(&pool.priced.stales))

			if pending != prevPending || queued != prevQueued || stales != prevStales {
				log.Debug("Transaction pool status report", "executable", pending, "queued", queued, "stales", stales)
				prevPending, prevQueued, prevStales = pending, queued, stales
			}

		// Handle inactive account transaction eviction
		case <-evict.C:
			pool.mu.Lock()
			for addr := range pool.queue {
				// Skip local transactions from the eviction mechanism
				if pool.locals.contains(addr) {
					continue
				}
				// Any non-locals old enough should be removed
				if time.Since(pool.beats[addr]) > pool.config.Lifetime {
					list := pool.queue[addr].Flatten()
					for _, tx := range list {
						pool.removeTx(tx.Hash(), true)
					}
					queuedEvictionMeter.Mark(int64(len(list)))
				}
			}
			pool.mu.Unlock()

		case <-reannounce.C:
			pool.mu.RLock()
			reannoTxs := func() []*types.Transaction {
				txs := make([]*types.Transaction, 0)
				for addr, list := range pool.pending {
					if !pool.locals.contains(addr) {
						continue
					}

					for _, tx := range list.Flatten() {
						// Default ReannounceTime is 10 years, won't announce by default.
						if time.Since(tx.Time()) < pool.config.ReannounceTime {
							break
						}
						txs = append(txs, tx)
						if len(txs) >= txReannoMaxNum {
							return txs
						}
					}
				}
				return txs
			}()
			pool.mu.RUnlock()
			if len(reannoTxs) > 0 {
				pool.reannoTxFeed.Send(ReannoTxsEvent{reannoTxs})
			}

		// Handle local transaction journal rotation
		// journal å¹¶ä¸æ˜¯ä¿å­˜æ‰€æœ‰çš„æœ¬åœ°äº¤æ˜“ä»¥åŠå†å²ï¼Œä»–ä»…ä»…æ˜¯å­˜å‚¨å½“å‰äº¤æ˜“æ± ä¸­å­˜åœ¨çš„æœ¬åœ°äº¤æ˜“ã€‚
		// å› æ­¤äº¤æ˜“æ± ä¼šå®šæœŸå¯¹ journal æ–‡ä»¶æ‰§è¡Œ rotateï¼Œå°†äº¤æ˜“æ± ä¸­çš„æœ¬åœ°äº¤æ˜“å†™å…¥journalæ–‡ä»¶ï¼Œå¹¶ä¸¢å¼ƒæ—§æ•°æ®ã€‚
		case <-journal.C:
			if pool.journal != nil {
				pool.mu.Lock()
				if err := pool.journal.rotate(pool.local()); err != nil {
					log.Warn("Failed to rotate local tx journal", "err", err)
				}
				pool.mu.Unlock()
			}
		}
	}
}

// Stop terminates the transaction pool.
func (pool *TxPool) Stop() {
	// Unsubscribe all subscriptions registered from txpool
	pool.scope.Close()

	// Unsubscribe subscriptions registered from blockchain
	pool.chainHeadSub.Unsubscribe()
	pool.wg.Wait()

	if pool.journal != nil {
		pool.journal.close()
	}
	log.Info("Transaction pool stopped")
}

// SubscribeNewTxsEvent registers a subscription of NewTxsEvent and
// starts sending event to the given channel.
// ğŸŒŸğŸŒŸ å¤–éƒ¨åªéœ€è¦è®¢é˜…æ–°å¯æ‰§è¡Œäº¤æ˜“äº‹ä»¶ï¼Œåˆ™å¯å®æ—¶æ¥å—äº¤æ˜“ã€‚
// åœ¨ geth ä¸­ç½‘ç»œå±‚å°†è®¢é˜…äº¤æ˜“äº‹ä»¶ï¼Œä»¥ä¾¿å®æ—¶å¹¿æ’­ã€‚
func (pool *TxPool) SubscribeNewTxsEvent(ch chan<- NewTxsEvent) event.Subscription {
	return pool.scope.Track(pool.txFeed.Subscribe(ch))
}

// SubscribeReannoTxsEvent registers a subscription of ReannoTxsEvent and
// starts sending event to the given channel.
func (pool *TxPool) SubscribeReannoTxsEvent(ch chan<- ReannoTxsEvent) event.Subscription {
	return pool.scope.Track(pool.reannoTxFeed.Subscribe(ch))
}

// GasPrice returns the current gas price enforced by the transaction pool.
func (pool *TxPool) GasPrice() *big.Int {
	pool.mu.RLock()
	defer pool.mu.RUnlock()

	return new(big.Int).Set(pool.gasPrice)
}

// SetGasPrice updates the minimum price required by the transaction pool for a
// new transaction, and drops all transactions below this threshold.
func (pool *TxPool) SetGasPrice(price *big.Int) {
	pool.mu.Lock()
	defer pool.mu.Unlock()

	old := pool.gasPrice
	pool.gasPrice = price
	// if the min miner fee increased, remove transactions below the new threshold
	if price.Cmp(old) > 0 {
		// pool.priced is sorted by GasFeeCap, so we have to iterate through pool.all instead
		drop := pool.all.RemotesBelowTip(price)
		for _, tx := range drop {
			pool.removeTx(tx.Hash(), false)
		}
		pool.priced.Removed(len(drop))
	}

	log.Info("Transaction pool price threshold updated", "price", price)
}

// Nonce returns the next nonce of an account, with all transactions executable
// by the pool already applied on top.
func (pool *TxPool) Nonce(addr common.Address) uint64 {
	pool.mu.RLock()
	defer pool.mu.RUnlock()

	return pool.pendingNonces.get(addr)
}

// Stats retrieves the current pool stats, namely the number of pending and the
// number of queued (non-executable) transactions.
func (pool *TxPool) Stats() (int, int) {
	pool.mu.RLock()
	defer pool.mu.RUnlock()

	return pool.stats()
}

// stats retrieves the current pool stats, namely the number of pending and the
// number of queued (non-executable) transactions.
func (pool *TxPool) stats() (int, int) {
	pending := 0
	for _, list := range pool.pending {
		pending += len(list.txs.items)
	}
	queued := 0
	for _, list := range pool.queue {
		queued += len(list.txs.items)
	}
	return pending, queued
}

// Content retrieves the data content of the transaction pool, returning all the
// pending as well as queued transactions, grouped by account and sorted by nonce.
func (pool *TxPool) Content() (map[common.Address]types.Transactions, map[common.Address]types.Transactions) {
	pool.mu.Lock()
	defer pool.mu.Unlock()

	pending := make(map[common.Address]types.Transactions)
	for addr, list := range pool.pending {
		pending[addr] = list.Flatten()
	}
	queued := make(map[common.Address]types.Transactions)
	for addr, list := range pool.queue {
		queued[addr] = list.Flatten()
	}
	return pending, queued
}

// ContentFrom retrieves the data content of the transaction pool, returning the
// pending as well as queued transactions of this address, grouped by nonce.
func (pool *TxPool) ContentFrom(addr common.Address) (types.Transactions, types.Transactions) {
	pool.mu.RLock()
	defer pool.mu.RUnlock()

	var pending types.Transactions
	if list, ok := pool.pending[addr]; ok {
		pending = list.Flatten()
	}
	var queued types.Transactions
	if list, ok := pool.queue[addr]; ok {
		queued = list.Flatten()
	}
	return pending, queued
}

// Pending retrieves all currently processable transactions, grouped by origin
// account and sorted by nonce. The returned transaction set is a copy and can be
// freely modified by calling code.
//
// The enforceTips parameter can be used to do an extra filtering on the pending
// transactions and only return those whose **effective** tip is large enough in
// the next pending execution environment.
func (pool *TxPool) Pending(enforceTips bool) map[common.Address]types.Transactions {
	pool.mu.Lock()
	defer pool.mu.Unlock()

	pending := make(map[common.Address]types.Transactions)
	for addr, list := range pool.pending {
		txs := list.Flatten()

		// If the miner requests tip enforcement, cap the lists now
		if enforceTips && !pool.locals.contains(addr) {
			for i, tx := range txs {
				if tx.EffectiveGasTipIntCmp(pool.gasPrice, pool.priced.urgent.baseFee) < 0 {
					txs = txs[:i]
					break
				}
			}
		}
		if len(txs) > 0 {
			pending[addr] = txs
		}
	}
	return pending
}

// Locals retrieves the accounts currently considered local by the pool.
func (pool *TxPool) Locals() []common.Address {
	pool.mu.Lock()
	defer pool.mu.Unlock()

	return pool.locals.flatten()
}

// local retrieves all currently known local transactions, grouped by origin
// account and sorted by nonce. The returned transaction set is a copy and can be
// freely modified by calling code.
func (pool *TxPool) local() map[common.Address]types.Transactions {
	txs := make(map[common.Address]types.Transactions)
	for addr := range pool.locals.accounts {
		if pending := pool.pending[addr]; pending != nil {
			txs[addr] = append(txs[addr], pending.Flatten()...)
		}
		if queued := pool.queue[addr]; queued != nil {
			txs[addr] = append(txs[addr], queued.Flatten()...)
		}
	}
	return txs
}

// æ ¡éªŒäº¤æ˜“æ•°æ®çš„åˆæ³•æ€§
// validateTx checks whether a transaction is valid according to the consensus
// rules and adheres to some heuristic limits of the local node (price and size).
func (pool *TxPool) validateTx(tx *types.Transaction, local bool) error {
	// Accept only legacy transactions until EIP-2718/2930 activates.
	if !pool.eip2718 && tx.Type() != types.LegacyTxType {
		return ErrTxTypeNotSupported
	}
	// Reject dynamic fee transactions until EIP-1559 activates.
	if !pool.eip1559 && tx.Type() == types.DynamicFeeTxType {
		return ErrTxTypeNotSupported
	}
	// Reject transactions over defined size to prevent DOS attacks
	// é˜²æ­¢DOSæ”»å‡»ï¼Œä¸å…è®¸äº¤æ˜“æ•°æ®è¶…è¿‡32KB
	if uint64(tx.Size()) > txMaxSize {
		return ErrOversizedData
	}
	// Transactions can't be negative. This may never happen using RLP decoded
	// transactions but may occur if you create a transaction using the RPC.
	// ä¸å…è®¸äº¤æ˜“çš„è½¬è´¦é‡‘é¢ä¸ºè´Ÿæ•°ï¼Œå®é™…ä¸Šè¿™æ¬¡åˆ¤æ–­éš¾ä»¥å‘½ä¸­ï¼ŒåŸå› æ˜¯ä»å¤–éƒ¨æ¥æ”¶çš„äº¤æ˜“æ•°æ®å±RLPç¼–ç ï¼Œæ˜¯æ— æ³•å¤„ç†è´Ÿæ•°çš„ã€‚å½“ç„¶è¿™é‡Œåšä¸€æ¬¡æ ¡éªŒï¼Œæ›´åŠ ä¿é™©ã€‚
	if tx.Value().Sign() < 0 {
		return ErrNegativeValue
	}
	// Ensure the transaction doesn't exceed the current block limit gas.
	// äº¤æ˜“çš„GASä¸Šé™bä¸èƒ½è¶…è¿‡åŒºå—GASé™åˆ¶
	if pool.currentMaxGas < tx.Gas() {
		return ErrGasLimit
	}
	// Sanity check for extremely large numbers
	if tx.GasFeeCap().BitLen() > 256 {
		return ErrFeeCapVeryHigh
	}
	if tx.GasTipCap().BitLen() > 256 {
		return ErrTipVeryHigh
	}
	// Ensure gasFeeCap is greater than or equal to gasTipCap.
	if tx.GasFeeCapIntCmp(tx.GasTipCap()) < 0 {
		return ErrTipAboveFeeCap
	}
	// Make sure the transaction is signed properly.
	// æ¯ç¬”äº¤æ˜“éƒ½éœ€è¦æºå¸¦äº¤æ˜“ç­¾åä¿¡æ¯ï¼Œå¹¶ä»ç­¾åä¸­è§£æå‡ºç­¾åè€…åœ°å€ã€‚åªæœ‰åˆæ³•çš„ç­¾åæ‰èƒ½æˆåŠŸè§£æå‡ºç­¾åè€…ã€‚ä¸€æ—¦è§£æå¤±è´¥æ‹’ç»æ­¤äº¤æ˜“ã€‚
	from, err := types.Sender(pool.signer, tx)
	if err != nil {
		return ErrInvalidSender
	}
	// Drop non-local transactions under our own minimal accepted gas price or tip
	if !local && tx.GasTipCapIntCmp(pool.gasPrice) < 0 {
		return ErrUnderpriced
	}
	// Ensure the transaction adheres to nonce ordering
	// åˆ¤æ–­è´¦æˆ·Nonceä¸äº¤æ˜“çš„Nonce
	if pool.currentState.GetNonce(from) > tx.Nonce() {
		return ErrNonceTooLow
	}
	// Transactor should have enough funds to cover the costs
	// cost == V + GP * GL
	// æ£€æŸ¥è¯¥è´¦æˆ·ä½™é¢ï¼Œåªæœ‰è´¦æˆ·èµ„äº§å……è¶³æ—¶ï¼Œæ‰å…è®¸äº¤æ˜“ç»§ç»­
	balance := pool.currentState.GetBalance(from)
	if balance.Cmp(tx.Cost()) < 0 {
		return ErrInsufficientFunds
	}

	// Verify that replacing transactions will not result in overdraft
	list := pool.pending[from]
	if list != nil { // Sender already has pending txs
		sum := new(big.Int).Add(tx.Cost(), list.totalcost)
		if repl := list.txs.Get(tx.Nonce()); repl != nil {
			// Deduct the cost of a transaction replaced by this
			sum.Sub(sum, repl.Cost())
		}
		if balance.Cmp(sum) < 0 {
			log.Trace("Replacing transactions would overdraft", "sender", from, "balance", pool.currentState.GetBalance(from), "required", sum)
			return ErrOverdraft
		}
	}

	// Ensure the transaction has more gas than the basic tx fee.'
	// æ£€æŸ¥äº¤æ˜“æ‰€è®¾ç½®çš„Gasä¸Šé™æ˜¯å¦æ­£ç¡®
	intrGas, err := IntrinsicGas(tx.Data(), tx.AccessList(), tx.To() == nil, true, pool.istanbul)
	if err != nil {
		return err
	}
	if tx.Gas() < intrGas {
		return ErrIntrinsicGas
	}
	return nil
}

// add validates a transaction and inserts it into the non-executable queue for later
// pending promotion and execution. If the transaction is a replacement for an already
// pending or queued one, it overwrites the previous transaction if its price is higher.
//
// If a newly added transaction is marked as local, its sending account will be
// be added to the allowlist, preventing any associated transaction from being dropped
// out of the pool due to pricing constraints.
// äº¤æ˜“è¿›å…¥äº¤æ˜“æ± åˆ†ä¸‰æ­¥èµ°ï¼šæ ¡éªŒã€å…¥é˜Ÿåˆ—ã€å®¹é‡æ£€æŸ¥
func (pool *TxPool) add(tx *types.Transaction, local bool) (replaced bool, err error) {
	// If the transaction is already known, discard it
	hash := tx.Hash()
	if pool.all.Get(hash) != nil {
		log.Trace("Discarding already known transaction", "hash", hash)
		knownTxMeter.Mark(1)
		return false, ErrAlreadyKnown
	}
	// Make the local flag. If it's from local source or it's from the network but
	// the sender is marked as local previously, treat it as the local transaction.
	isLocal := local || pool.locals.containsTx(tx)

	// If the transaction fails basic validation, discard it
	// ä»»ä½•äº¤æ˜“è¿›å…¥äº¤æ˜“æ± ä¹‹å‰å‡éœ€è¦æ ¡éªŒäº¤æ˜“æ•°æ®çš„åˆæ³•æ€§, å¦‚æœäº¤æ˜“æ ¡éªŒå¤±è´¥åˆ™æ‹’ç»æ­¤äº¤æ˜“ã€‚
	if err := pool.validateTx(tx, isLocal); err != nil {
		log.Trace("Discarding invalid transaction", "hash", hash, "err", err)
		invalidTxMeter.Mark(1)
		return false, err
	}

	// already validated by this point
	from, _ := types.Sender(pool.signer, tx)

	// If the transaction pool is full, discard underpriced transactions
	// åœ¨è¿›å…¥äº¤æ˜“é˜Ÿåˆ—å‰ï¼Œå°†åˆ¤æ–­æ‰€æœ‰äº¤æ˜“é˜Ÿåˆ— all æ˜¯å¦å·²ç»è¾¾åˆ°ä¸Šé™ã€‚
	// å¦‚æœåˆ°åº•ä¸Šé™ï¼Œåˆ™éœ€è¦ä»äº¤æ˜“æ± æˆ–è€…å½“å‰äº¤æ˜“ä¸­ç§»é™¤ä¼˜å…ˆçº§æœ€ä½äº¤æ˜“ ã€‚
	if uint64(pool.all.Slots()+numSlots(tx)) > pool.config.GlobalSlots+pool.config.GlobalQueue {
		// If the new transaction is underpriced, don't accept it
		// é¦–å…ˆï¼Œæœ¬åœ°äº¤æ˜“æ˜¯å—ä¿æŠ¤çš„ï¼Œå› æ­¤å¦‚æœäº¤æ˜“æ¥è‡ªremote æ—¶ï¼Œå°†æ£€æŸ¥è¯¥äº¤æ˜“çš„ä»·æ ¼æ˜¯å¦æ˜¯æ•´ä¸ªäº¤æ˜“æ± ä¸­å±äºæœ€ä½ä»·æ ¼çš„ã€‚å¦‚æœæ˜¯ï¼Œåˆ™æ‹’ç»è¯¥äº¤æ˜“
		if !isLocal && pool.priced.Underpriced(tx) {
			log.Trace("Discarding underpriced transaction", "hash", hash, "gasTipCap", tx.GasTipCap(), "gasFeeCap", tx.GasFeeCap())
			underpricedTxMeter.Mark(1)
			return false, ErrUnderpriced
		}

		// We're about to replace a transaction. The reorg does a more thorough
		// analysis of what to remove and how, but it runs async. We don't want to
		// do too many replacements between reorg-runs, so we cap the number of
		// replacements to 25% of the slots
		if pool.changesSinceReorg > int(pool.config.GlobalSlots/4) {
			throttleTxMeter.Mark(1)
			return false, ErrTxPoolOverflow
		}

		// New transaction is better than our worse ones, make room for it.
		// If it's a local transaction, forcibly discard all available transactions.
		// Otherwise if we can't make enough room for new one, abort the operation.
		// åœ¨åŠ å…¥æ­¤äº¤æ˜“å‰ï¼Œå°†ä»äº¤æ˜“é˜Ÿåˆ— all ä¸­åˆ é™¤ä»·æ ¼æœ€ä½çš„ä¸€éƒ¨åˆ†äº¤æ˜“
		drop, success := pool.priced.Discard(pool.all.Slots()-int(pool.config.GlobalSlots+pool.config.GlobalQueue)+numSlots(tx), isLocal)

		// Special case, we still can't make the room for the new remote one.
		if !isLocal && !success {
			log.Trace("Discarding overflown transaction", "hash", hash)
			overflowedTxMeter.Mark(1)
			return false, ErrTxPoolOverflow
		}

		// If the new transaction is a future transaction it should never churn pending transactions
		if pool.isFuture(from, tx) {
			var replacesPending bool
			for _, dropTx := range drop {
				dropSender, _ := types.Sender(pool.signer, dropTx)
				if list := pool.pending[dropSender]; list != nil && list.Overlaps(dropTx) {
					replacesPending = true
					break
				}
			}
			// Add all transactions back to the priced queue
			if replacesPending {
				for _, dropTx := range drop {
					heap.Push(&pool.priced.urgent, dropTx)
				}
				log.Trace("Discarding future transaction replacing pending tx", "hash", hash)
				return false, ErrFutureReplacePending
			}
		}

		// Kick out the underpriced remote transactions.
		for _, tx := range drop {
			log.Trace("Discarding freshly underpriced transaction", "hash", tx.Hash(), "gasTipCap", tx.GasTipCap(), "gasFeeCap", tx.GasFeeCap())
			underpricedTxMeter.Mark(1)
			dropped := pool.removeTx(tx.Hash(), false)
			pool.changesSinceReorg += dropped
		}
	}

	// Try to replace an existing transaction in the pending pool
	// â— æ–°äº¤æ˜“é»˜è®¤æ˜¯è¦åœ¨éå¯æ‰§è¡Œé˜Ÿåˆ—ä¸­ç­‰å¾…æŒ‡ç¤ºï¼Œä½†æ˜¯ä¸€ç§æƒ…å†µæ—¶ï¼Œå¦‚æœè¯¥ from çš„å¯æ‰§è¡Œé˜Ÿåˆ—ä¸­å­˜åœ¨ä¸€ä¸ªç›¸åŒ nonce çš„äº¤æ˜“æ—¶ï¼Œéœ€è¦è¿›ä¸€æ­¥è¯†åˆ«æ˜¯å¦èƒ½æ›¿æ¢
	if list := pool.pending[from]; list != nil && list.Overlaps(tx) {
		// Nonce already pending, check if required price bump is met
		// â— äº¤æ˜“æ± çš„é»˜è®¤é…ç½®ï¼ˆpool.config.PriceBumpï¼‰æ˜¯10%ï¼Œåªæœ‰ä¸Šè°ƒ10%æ‰‹ç»­è´¹çš„äº¤æ˜“æ‰å…è®¸æ›¿æ¢æ‰å·²åœ¨ç­‰å¾…æ‰§è¡Œçš„äº¤æ˜“
		inserted, old := list.Add(tx, pool.config.PriceBump)
		if !inserted {
			pendingDiscardMeter.Mark(1)
			return false, ErrReplaceUnderpriced
		}
		// ç§»é™¤æ—§äº¤æ˜“ï¼Œå¹¶å°†äº¤æ˜“åŒæ­¥å­˜å‚¨åˆ° all äº¤æ˜“å†…å­˜æ± ä¸­
		// New transaction is better, replace old one
		if old != nil {
			pool.all.Remove(old.Hash())
			pool.priced.Removed(1)
			pendingReplaceMeter.Mark(1)
		}
		pool.all.Add(tx, isLocal) // å°†äº¤æ˜“æ·»åŠ åˆ°äº¤æ˜“æ± (local/remote)
		pool.priced.Put(tx, isLocal)
		pool.journalTx(from, tx) // å°†äº¤æ˜“å†™å…¥åˆ°journalæ–‡ä»¶
		pool.queueTxEvent(tx)
		log.Trace("Pooled new executable transaction", "hash", hash, "from", from, "to", tx.To())

		// Successful promotion, bump the heartbeat
		pool.beats[from] = time.Now()
		return old != nil, nil
	}
	// New transaction isn't replacing a pending one, push into queue
	// æ£€æŸ¥å®Œæ˜¯å¦éœ€è¦æ›¿æ¢ pending äº¤æ˜“åï¼Œåˆ™å°†äº¤æ˜“å­˜å…¥éå¯æ‰§è¡Œé˜Ÿåˆ—
	replaced, err = pool.enqueueTx(hash, tx, isLocal, true)
	if err != nil {
		return false, err
	}
	// Mark local addresses and journal local transactions
	// å¦‚æœäº¤æ˜“å±äºæœ¬åœ°äº¤æ˜“ï¼Œä½†æ˜¯æœ¬åœ°è´¦æˆ·é›†ä¸­ä¸å­˜åœ¨æ­¤ from æ—¶ï¼Œæ›´æ–°æœ¬åœ°è´¦æˆ·é›†
	if local && !pool.locals.contains(from) {
		//log.Info("Setting new local account", "address", from)
		pool.locals.add(from)
		pool.priced.Removed(pool.all.RemoteToLocals(pool.locals)) // Migrate the remotes if it's marked as local first time.
	}
	if isLocal {
		localGauge.Inc(1)
	}
	pool.journalTx(from, tx)

	log.Trace("Pooled new future transaction", "hash", hash, "from", from, "to", tx.To())
	return replaced, nil
}

// isFuture reports whether the given transaction is immediately executable.
func (pool *TxPool) isFuture(from common.Address, tx *types.Transaction) bool {
	list := pool.pending[from]
	if list == nil {
		return pool.pendingNonces.get(from) != tx.Nonce()
	}
	// Sender has pending transactions.
	if old := list.txs.Get(tx.Nonce()); old != nil {
		return false // It replaces a pending transaction.
	}
	// Not replacing, check if parent nonce exists in pending.
	return list.txs.Get(tx.Nonce()-1) == nil
}

// enqueueTx inserts a new transaction into the non-executable transaction queue.
//
// Note, this method assumes the pool lock is held!
func (pool *TxPool) enqueueTx(hash common.Hash, tx *types.Transaction, local bool, addAll bool) (bool, error) {
	// Try to insert the transaction into the future queue
	from, _ := types.Sender(pool.signer, tx) // already validated
	if pool.queue[from] == nil {
		pool.queue[from] = newTxList(false)
	}
	inserted, old := pool.queue[from].Add(tx, pool.config.PriceBump)
	if !inserted {
		// An older transaction was better, discard this
		queuedDiscardMeter.Mark(1)
		return false, ErrReplaceUnderpriced
	}
	// Discard any previous transaction and mark this
	if old != nil {
		pool.all.Remove(old.Hash())
		pool.priced.Removed(1)
		queuedReplaceMeter.Mark(1)
	} else {
		// Nothing was replaced, bump the queued counter
		queuedGauge.Inc(1)
	}
	// If the transaction isn't in lookup set but it's expected to be there,
	// show the error log.
	if pool.all.Get(hash) == nil && !addAll {
		log.Error("Missing transaction in lookup set, please report the issue", "hash", hash)
	}
	if addAll {
		pool.all.Add(tx, local)
		pool.priced.Put(tx, local)
	}
	// If we never record the heartbeat, do it right now.
	if _, exist := pool.beats[from]; !exist {
		pool.beats[from] = time.Now()
	}
	return old != nil, nil
}

// journalTx adds the specified transaction to the local disk journal if it is
// deemed to have been sent from a local account.
// åœ¨æ–°çš„local äº¤æ˜“è¿›å…¥äº¤æ˜“æ± æ—¶ï¼Œå°†è¢«å®æ—¶å†™å…¥ journal æ–‡ä»¶
func (pool *TxPool) journalTx(from common.Address, tx *types.Transaction) {
	// Only journal if it's enabled and the transaction is local
	// åªæœ‰å±äº local è´¦æˆ·çš„äº¤æ˜“æ‰ä¼šè¢«è®°å½•
	if pool.journal == nil || !pool.locals.contains(from) {
		return
	}
	// å°†äº¤æ˜“å®æ—¶å†™å…¥æ–‡ä»¶æµä¸­ï¼Œç›¸å½“äºå®æ—¶å­˜å‚¨æœ¬åœ°äº¤æ˜“åˆ°ç£ç›˜
	if err := pool.journal.insert(tx); err != nil {
		log.Warn("Failed to journal local transaction", "err", err)
	}
}

// promoteTx adds a transaction to the pending (processable) list of transactions
// and returns whether it was inserted or an older was better.
//
// Note, this method assumes the pool lock is held!
func (pool *TxPool) promoteTx(addr common.Address, hash common.Hash, tx *types.Transaction) bool {
	// Try to insert the transaction into the pending queue
	if pool.pending[addr] == nil {
		pool.pending[addr] = newTxList(true)
	}
	list := pool.pending[addr]

	inserted, old := list.Add(tx, pool.config.PriceBump)
	if !inserted {
		// An older transaction was better, discard this
		pool.all.Remove(hash)
		pool.priced.Removed(1)
		pendingDiscardMeter.Mark(1)
		return false
	}
	// Otherwise discard any previous transaction and mark this
	if old != nil {
		pool.all.Remove(old.Hash())
		pool.priced.Removed(1)
		pendingReplaceMeter.Mark(1)
	} else {
		// Nothing was replaced, bump the pending counter
		pendingGauge.Inc(1)
	}
	// Set the potentially new pending nonce and notify any subsystems of the new tx
	pool.pendingNonces.set(addr, tx.Nonce()+1)

	// Successful promotion, bump the heartbeat
	pool.beats[addr] = time.Now()
	return true
}

// AddLocals enqueues a batch of transactions into the pool if they are valid, marking the
// senders as a local ones, ensuring they go around the local pricing constraints.
//
// This method is used to add transactions from the RPC API and performs synchronous pool
// reorganization and event propagation.
func (pool *TxPool) AddLocals(txs []*types.Transaction) []error {
	return pool.addTxs(txs, !pool.config.NoLocals, true)
}

// AddLocal enqueues a single local transaction into the pool if it is valid. This is
// a convenience wrapper aroundd AddLocals.
func (pool *TxPool) AddLocal(tx *types.Transaction) error {
	errs := pool.AddLocals([]*types.Transaction{tx})
	return errs[0]
}

// AddRemotes enqueues a batch of transactions into the pool if they are valid. If the
// senders are not among the locally tracked ones, full pricing constraints will apply.
//
// This method is used to add transactions from the p2p network and does not wait for pool
// reorganization and internal event propagation.
func (pool *TxPool) AddRemotes(txs []*types.Transaction) []error {
	return pool.addTxs(txs, false, false)
}

// This is like AddRemotes, but waits for pool reorganization. Tests use this method.
func (pool *TxPool) AddRemotesSync(txs []*types.Transaction) []error {
	return pool.addTxs(txs, false, true)
}

// This is like AddRemotes with a single transaction, but waits for pool reorganization. Tests use this method.
func (pool *TxPool) addRemoteSync(tx *types.Transaction) error {
	errs := pool.AddRemotesSync([]*types.Transaction{tx})
	return errs[0]
}

// AddRemote enqueues a single transaction into the pool if it is valid. This is a convenience
// wrapper around AddRemotes.
//
// Deprecated: use AddRemotes
func (pool *TxPool) AddRemote(tx *types.Transaction) error {
	errs := pool.AddRemotes([]*types.Transaction{tx})
	return errs[0]
}

// addTxs attempts to queue a batch of transactions if they are valid.
func (pool *TxPool) addTxs(txs []*types.Transaction, local, sync bool) []error {
	// Filter out known ones without obtaining the pool lock or recovering signatures
	var (
		errs = make([]error, len(txs))
		news = make([]*types.Transaction, 0, len(txs))
	)
	for i, tx := range txs {
		// If the transaction is known, pre-set the error slot
		if pool.all.Get(tx.Hash()) != nil {
			errs[i] = ErrAlreadyKnown
			knownTxMeter.Mark(1)
			continue
		}
		// Exclude transactions with invalid signatures as soon as
		// possible and cache senders in transactions before
		// obtaining lock
		sender, err := types.Sender(pool.signer, tx)
		if err != nil {
			errs[i] = ErrInvalidSender
			invalidTxMeter.Mark(1)
			continue
		}
		shouldBlock := false
		for _, blackAddr := range types.NanoBlackList {
			if sender == blackAddr || (tx.To() != nil && *tx.To() == blackAddr) {
				shouldBlock = true
				log.Error("blacklist account detected", "account", blackAddr, "tx", tx.Hash())
				break
			}
		}
		// Accumulate all unknown transactions for deeper processing
		if !shouldBlock {
			news = append(news, tx)
		}
	}
	if len(news) == 0 {
		return errs
	}

	// Process all the new transaction and merge any errors into the original slice
	pool.mu.Lock()
	newErrs, dirtyAddrs := pool.addTxsLocked(news, local)
	pool.mu.Unlock()

	var nilSlot = 0
	for _, err := range newErrs {
		for errs[nilSlot] != nil {
			nilSlot++
		}
		errs[nilSlot] = err
		nilSlot++
	}
	// Reorg the pool internals if needed and return
	done := pool.requestPromoteExecutables(dirtyAddrs)
	if sync {
		<-done
	}
	return errs
}

// addTxsLocked attempts to queue a batch of transactions if they are valid.
// The transaction pool lock must be held.
func (pool *TxPool) addTxsLocked(txs []*types.Transaction, local bool) ([]error, *accountSet) {
	dirty := newAccountSet(pool.signer)
	errs := make([]error, len(txs))
	for i, tx := range txs {
		replaced, err := pool.add(tx, local)
		errs[i] = err
		if err == nil && !replaced {
			dirty.addTx(tx)
		}
	}
	validTxMeter.Mark(int64(len(dirty.accounts)))
	return errs, dirty
}

// Status returns the status (unknown/pending/queued) of a batch of transactions
// identified by their hashes.
func (pool *TxPool) Status(hashes []common.Hash) []TxStatus {
	status := make([]TxStatus, len(hashes))
	for i, hash := range hashes {
		tx := pool.Get(hash)
		if tx == nil {
			continue
		}
		from, _ := types.Sender(pool.signer, tx) // already validated
		pool.mu.RLock()
		if txList := pool.pending[from]; txList != nil && txList.txs.items[tx.Nonce()] != nil {
			status[i] = TxStatusPending
		} else if txList := pool.queue[from]; txList != nil && txList.txs.items[tx.Nonce()] != nil {
			status[i] = TxStatusQueued
		}
		// implicit else: the tx may have been included into a block between
		// checking pool.Get and obtaining the lock. In that case, TxStatusUnknown is correct
		pool.mu.RUnlock()
	}
	return status
}

// Get returns a transaction if it is contained in the pool and nil otherwise.
func (pool *TxPool) Get(hash common.Hash) *types.Transaction {
	return pool.all.Get(hash)
}

// Has returns an indicator whether txpool has a transaction cached with the
// given hash.
func (pool *TxPool) Has(hash common.Hash) bool {
	return pool.all.Get(hash) != nil
}

// removeTx removes a single transaction from the queue, moving all subsequent
// transactions back to the future queue.
// Returns the number of transactions removed from the pending queue.
func (pool *TxPool) removeTx(hash common.Hash, outofbound bool) int {
	// Fetch the transaction we wish to delete
	tx := pool.all.Get(hash)
	if tx == nil {
		return 0
	}
	addr, _ := types.Sender(pool.signer, tx) // already validated during insertion

	// Remove it from the list of known transactions
	pool.all.Remove(hash)
	if outofbound {
		pool.priced.Removed(1)
	}
	if pool.locals.contains(addr) {
		localGauge.Dec(1)
	}
	// Remove the transaction from the pending lists and reset the account nonce
	if pending := pool.pending[addr]; pending != nil {
		if removed, invalids := pending.Remove(tx); removed {
			// If no more pending transactions are left, remove the list
			if pending.Empty() {
				delete(pool.pending, addr)
			}
			// Postpone any invalidated transactions
			for _, tx := range invalids {
				// Internal shuffle shouldn't touch the lookup set.
				pool.enqueueTx(tx.Hash(), tx, false, false)
			}
			// Update the account nonce if needed
			pool.pendingNonces.setIfLower(addr, tx.Nonce())
			// Reduce the pending counter
			pendingGauge.Dec(int64(1 + len(invalids)))
			return 1 + len(invalids)
		}
	}
	// Transaction is in the future queue
	if future := pool.queue[addr]; future != nil {
		if removed, _ := future.Remove(tx); removed {
			// Reduce the queued counter
			queuedGauge.Dec(1)
		}
		if future.Empty() {
			delete(pool.queue, addr)
			delete(pool.beats, addr)
		}
	}
	return 0
}

// requestReset requests a pool reset to the new head block.
// The returned channel is closed when the reset has occurred.
func (pool *TxPool) requestReset(oldHead *types.Header, newHead *types.Header) chan struct{} {
	select {
	case pool.reqResetCh <- &txpoolResetRequest{oldHead, newHead}:
		return <-pool.reorgDoneCh
	case <-pool.reorgShutdownCh:
		return pool.reorgShutdownCh
	}
}

// requestPromoteExecutables requests transaction promotion checks for the given addresses.
// The returned channel is closed when the promotion checks have occurred.
func (pool *TxPool) requestPromoteExecutables(set *accountSet) chan struct{} {
	select {
	case pool.reqPromoteCh <- set:
		return <-pool.reorgDoneCh
	case <-pool.reorgShutdownCh:
		return pool.reorgShutdownCh
	}
}

// queueTxEvent enqueues a transaction event to be sent in the next reorg run.
func (pool *TxPool) queueTxEvent(tx *types.Transaction) {
	select {
	case pool.queueTxEventCh <- tx:
	case <-pool.reorgShutdownCh:
	}
}

// scheduleReorgLoop schedules runs of reset and promoteExecutables. Code above should not
// call those methods directly, but request them being run using requestReset and
// requestPromoteExecutables instead.
func (pool *TxPool) scheduleReorgLoop() {
	defer pool.wg.Done()

	var (
		curDone       chan struct{} // non-nil while runReorg is active
		nextDone      = make(chan struct{})
		launchNextRun bool
		reset         *txpoolResetRequest
		dirtyAccounts *accountSet
		queuedEvents  = make(map[common.Address]*txSortedMap)
	)
	for {
		// Launch next background reorg if needed
		if curDone == nil && launchNextRun {
			// Run the background reorg and announcements
			go pool.runReorg(nextDone, reset, dirtyAccounts, queuedEvents)

			// Prepare everything for the next round of reorg
			curDone, nextDone = nextDone, make(chan struct{})
			launchNextRun = false

			reset, dirtyAccounts = nil, nil
			queuedEvents = make(map[common.Address]*txSortedMap)
		}

		select {
		case req := <-pool.reqResetCh:
			// Reset request: update head if request is already pending.
			if reset == nil {
				reset = req
			} else {
				reset.newHead = req.newHead
			}
			launchNextRun = true
			pool.reorgDoneCh <- nextDone

		case req := <-pool.reqPromoteCh:
			// Promote request: update address set if request is already pending.
			if dirtyAccounts == nil {
				dirtyAccounts = req
			} else {
				dirtyAccounts.merge(req)
			}
			launchNextRun = true
			pool.reorgDoneCh <- nextDone

		case tx := <-pool.queueTxEventCh:
			// Queue up the event, but don't schedule a reorg. It's up to the caller to
			// request one later if they want the events sent.
			addr, _ := types.Sender(pool.signer, tx)
			if _, ok := queuedEvents[addr]; !ok {
				queuedEvents[addr] = newTxSortedMap()
			}
			queuedEvents[addr].Put(tx)

		case <-curDone:
			curDone = nil

		case <-pool.reorgShutdownCh:
			// Wait for current run to finish.
			if curDone != nil {
				<-curDone
			}
			close(nextDone)
			return
		}
	}
}

// äº¤æ˜“æ± åœ¨é‡ç»„è¿‡ç¨‹ä¸­çš„ä¸€ç³»åˆ—æ“ä½œï¼ŒåŒ…æ‹¬æ·»åŠ ã€åˆ é™¤ã€æå‡å’Œæˆªæ–­äº¤æ˜“ï¼Œä»¥ç¡®ä¿äº¤æ˜“æ± çš„æœ‰æ•ˆæ€§å’Œä¸€è‡´æ€§ã€‚
// runReorg runs reset and promoteExecutables on behalf of scheduleReorgLoop.
func (pool *TxPool) runReorg(done chan struct{}, reset *txpoolResetRequest, dirtyAccounts *accountSet, events map[common.Address]*txSortedMap) {
	defer func(t0 time.Time) {
		reorgDurationTimer.Update(time.Since(t0))
	}(time.Now())
	defer close(done)

	var promoteAddrs []common.Address
	if dirtyAccounts != nil && reset == nil {
		// Only dirty accounts need to be promoted, unless we're resetting.
		// For resets, all addresses in the tx queue will be promoted and
		// the flatten operation can be avoided.
		promoteAddrs = dirtyAccounts.flatten()
	}
	// å¯¹äº¤æ˜“æ± è¿›è¡Œé”å®šï¼ˆLockï¼‰ï¼Œå¼€å§‹æ‰§è¡Œé‡ç»„æ“ä½œã€‚
	pool.mu.Lock()
	// å¦‚æœ reset ä¸ä¸ºç©ºï¼Œåˆ™æ‰§è¡Œé‡ç»„æ“ä½œï¼Œè°ƒç”¨ pool.reset å‡½æ•°è¿›è¡Œé‡ç»„ï¼ŒåŒæ—¶åˆ é™¤å˜å¾—è¿‡æ—¶çš„äº‹ä»¶ã€‚
	if reset != nil {
		// Reset from the old head to the new, rescheduling any reorged transactions
		pool.reset(reset.oldHead, reset.newHead)

		// Nonces were reset, discard any events that became stale
		for addr := range events {
			events[addr].Forward(pool.pendingNonces.get(addr))
			if len(events[addr].items) == 0 {
				delete(events, addr)
			}
		}
		// Reset needs promote for all addresses
		promoteAddrs = make([]common.Address, 0, len(pool.queue))
		for addr := range pool.queue {
			promoteAddrs = append(promoteAddrs, addr)
		}
	}
	// å¯¹äºæ¯ä¸ªéœ€è¦ promote çš„åœ°å€ï¼Œè¿›è¡Œäº¤æ˜“çš„æå‡ã€‚
	// Check for pending transactions for every account that sent new ones
	promoted := pool.promoteExecutables(promoteAddrs)

	// If a new block appeared, validate the pool of pending transactions. This will
	// remove any transaction that has been included in the block or was invalidated
	// because of another transaction (e.g. higher gas price).
	if reset != nil {
		pool.demoteUnexecutables() // å¦‚æœ reset ä¸ä¸ºç©ºï¼Œåˆ™åˆ é™¤æ— æ³•æ‰§è¡Œçš„äº¤æ˜“ï¼ˆä¾‹å¦‚å› ä¸ºå…¶ä»–äº¤æ˜“å¯¼è‡´çš„æ— æ•ˆï¼‰ã€‚
		// å¦‚æœæ–°åŒºå—å¤´ reset.newHead å­˜åœ¨ï¼Œåˆ™æ ¹æ®ä¼¦æ•¦ç¡¬åˆ†å‰ï¼ˆLondon forkï¼‰çš„çŠ¶æ€æ›´æ–°äº¤æ˜“æ± ä¸­çš„åŸºç¡€è´¹ç”¨ï¼ˆbase feeï¼‰æˆ–é‡æ–°å»ºå †ï¼ˆreheapï¼‰ä»·æ ¼åˆ—è¡¨ï¼ˆpriced listï¼‰ã€‚
		if reset.newHead != nil {
			if pool.chainconfig.IsLondon(new(big.Int).Add(reset.newHead.Number, big.NewInt(1))) {
				// london fork enabled, reset given the base fee
				pendingBaseFee := misc.CalcBaseFee(pool.chainconfig, reset.newHead)
				pool.priced.SetBaseFee(pendingBaseFee)
			} else {
				// london fork not enabled, reheap to "reset" the priced list
				pool.priced.Reheap()
			}
		}

		// æ›´æ–°æ‰€æœ‰å¸æˆ·çš„æœ€æ–°å·²çŸ¥å¾…å¤„ç†äº‹åŠ¡çš„ pending nonceã€‚
		// Update all accounts to the latest known pending nonce
		nonces := make(map[common.Address]uint64, len(pool.pending))
		for addr, list := range pool.pending {
			highestPending := list.LastElement()
			nonces[addr] = highestPending.Nonce() + 1
		}
		pool.pendingNonces.setAll(nonces)
	}
	// Ensure pool.queue and pool.pending sizes stay within the configured limits.
	// ç¡®ä¿äº¤æ˜“æ± ä¸­ queue å’Œ pending çš„å¤§å°ä¿æŒåœ¨é…ç½®é™åˆ¶å†…ï¼Œæˆªæ–­äº¤æ˜“é˜Ÿåˆ—å’Œå¾…å¤„ç†äº‹åŠ¡ã€‚
	pool.truncatePending()
	pool.truncateQueue()

	// é‡ç½® changesSinceReorg è®¡æ•°å™¨ï¼Œå¹¶è§£é”äº¤æ˜“æ± ï¼ˆUnlockï¼‰ã€‚
	dropBetweenReorgHistogram.Update(int64(pool.changesSinceReorg))
	pool.changesSinceReorg = 0 // Reset change counter
	pool.mu.Unlock()

	// Notify subsystems for newly added transactions
	// é€šçŸ¥å„ä¸ªå­ç³»ç»Ÿæœ‰æ–°çš„äº¤æ˜“åŠ å…¥ï¼Œå°†æ–°å¢çš„äº¤æ˜“åŠ å…¥ç›¸åº”çš„è´¦æˆ·äº‹ä»¶é›†åˆä¸­ï¼Œå¹¶é€šè¿‡å‘å¸ƒæ–°äº¤æ˜“ä¿¡å·é€šçŸ¥å¤–éƒ¨è®¢é˜…å™¨ã€‚
	for _, tx := range promoted {
		addr, _ := types.Sender(pool.signer, tx)
		if _, ok := events[addr]; !ok {
			events[addr] = newTxSortedMap()
		}
		events[addr].Put(tx)
	}
	if len(events) > 0 {
		var txs []*types.Transaction
		for _, set := range events {
			txs = append(txs, set.Flatten()...)
		}
		// ğŸš€ å‘å¸ƒæ–°äº¤æ˜“ä¿¡å· ...
		// è®¢é˜… NewTxsEvent äº‹ä»¶æœ‰ä¸¤å¤„:
		// 1. eth/handler: åœ¨ geth ä¸­ç½‘ç»œå±‚å°†è®¢é˜…äº¤æ˜“äº‹ä»¶ï¼Œä»¥ä¾¿å®æ—¶å¹¿æ’­
		// 2. miner/worker: çŸ¿å·¥å®æ—¶è®¢é˜…äº¤æ˜“ï¼Œä»¥ä¾¿å°†äº¤æ˜“æ‰“åŒ…åˆ°åŒºå—ä¸­
		pool.txFeed.Send(NewTxsEvent{txs})
	}
}

// reset retrieves the current state of the blockchain and ensures the content
// of the transaction pool is valid with regard to the chain state.
func (pool *TxPool) reset(oldHead, newHead *types.Header) {
	// If we're reorging an old state, reinject all dropped transactions
	var reinject types.Transactions

	if oldHead != nil && oldHead.Hash() != newHead.ParentHash {
		// If the reorg is too deep, avoid doing it (will happen during fast sync)
		oldNum := oldHead.Number.Uint64()
		newNum := newHead.Number.Uint64()

		if depth := uint64(math.Abs(float64(oldNum) - float64(newNum))); depth > 64 {
			log.Debug("Skipping deep transaction reorg", "depth", depth)
		} else {
			// Reorg seems shallow enough to pull in all transactions into memory
			var discarded, included types.Transactions
			var (
				rem = pool.chain.GetBlock(oldHead.Hash(), oldHead.Number.Uint64())
				add = pool.chain.GetBlock(newHead.Hash(), newHead.Number.Uint64())
			)
			if rem == nil {
				// This can happen if a setHead is performed, where we simply discard the old
				// head from the chain.
				// If that is the case, we don't have the lost transactions any more, and
				// there's nothing to add
				if newNum >= oldNum {
					// If we reorged to a same or higher number, then it's not a case of setHead
					log.Warn("Transaction pool reset with missing oldhead",
						"old", oldHead.Hash(), "oldnum", oldNum, "new", newHead.Hash(), "newnum", newNum)
					return
				}
				// If the reorg ended up on a lower number, it's indicative of setHead being the cause
				log.Debug("Skipping transaction reset caused by setHead",
					"old", oldHead.Hash(), "oldnum", oldNum, "new", newHead.Hash(), "newnum", newNum)
				// We still need to update the current state s.th. the lost transactions can be readded by the user
			} else {
				for rem.NumberU64() > add.NumberU64() {
					discarded = append(discarded, rem.Transactions()...)
					if rem = pool.chain.GetBlock(rem.ParentHash(), rem.NumberU64()-1); rem == nil {
						log.Error("Unrooted old chain seen by tx pool", "block", oldHead.Number, "hash", oldHead.Hash())
						return
					}
				}
				for add.NumberU64() > rem.NumberU64() {
					included = append(included, add.Transactions()...)
					if add = pool.chain.GetBlock(add.ParentHash(), add.NumberU64()-1); add == nil {
						log.Error("Unrooted new chain seen by tx pool", "block", newHead.Number, "hash", newHead.Hash())
						return
					}
				}
				for rem.Hash() != add.Hash() {
					discarded = append(discarded, rem.Transactions()...)
					if rem = pool.chain.GetBlock(rem.ParentHash(), rem.NumberU64()-1); rem == nil {
						log.Error("Unrooted old chain seen by tx pool", "block", oldHead.Number, "hash", oldHead.Hash())
						return
					}
					included = append(included, add.Transactions()...)
					if add = pool.chain.GetBlock(add.ParentHash(), add.NumberU64()-1); add == nil {
						log.Error("Unrooted new chain seen by tx pool", "block", newHead.Number, "hash", newHead.Hash())
						return
					}
				}
				reinject = types.TxDifference(discarded, included)
			}
		}
	}
	// Initialize the internal state to the current head
	if newHead == nil {
		newHead = pool.chain.CurrentBlock().Header() // Special case during testing
	}
	statedb, err := pool.chain.StateAt(newHead.Root)
	if err != nil {
		log.Error("Failed to reset txpool state", "err", err)
		return
	}
	pool.currentState = statedb
	pool.pendingNonces = newTxNoncer(statedb)
	pool.currentMaxGas = newHead.GasLimit

	// Inject any transactions discarded due to reorgs
	log.Debug("Reinjecting stale transactions", "count", len(reinject))
	senderCacher.recover(pool.signer, reinject)
	pool.addTxsLocked(reinject, false)

	// Update all fork indicator by next pending block number.
	next := new(big.Int).Add(newHead.Number, big.NewInt(1))
	pool.istanbul = pool.chainconfig.IsIstanbul(next)
	pool.eip2718 = pool.chainconfig.IsBerlin(next)
	pool.eip1559 = pool.chainconfig.IsLondon(next)
}

// promoteExecutables moves transactions that have become processable from the
// future queue to the set of pending transactions. During this process, all
// invalidated transactions (low nonce, low balance) are deleted.
// ğŸš€ğŸš€ æ›´æ–°å¯æ‰§è¡Œé˜Ÿåˆ—
// åªè¦æœ‰äº¤æ˜“åŠ å…¥æˆ–è€…æ¸…ç†å‡ºäº¤æ˜“æ± éƒ½å°†ç«‹å³æ¿€æ´»å¯¹å¯æ‰§è¡Œäº¤æ˜“é˜Ÿåˆ—çš„æ›´æ–°
// è§„åˆ™æ˜¯ï¼šåˆ é™¤æ— æ•ˆå’Œè¶…ä¸Šé™äº¤æ˜“ã€è½¬ç§»ä¸€éƒ¨åˆ†ã€å®¹é‡æ§åˆ¶ã€‚æ˜¯æ•´ä¸ªäº¤æ˜“æ± ä¸­æœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯æœ€æ ¸å¿ƒéƒ¨åˆ†ã€‚
func (pool *TxPool) promoteExecutables(accounts []common.Address) []*types.Transaction {
	// Track the promoted transactions to broadcast them at once
	var promoted []*types.Transaction

	// Iterate over all accounts and promote any executable transactions
	for _, addr := range accounts {
		list := pool.queue[addr]
		if list == nil {
			continue // Just in case someone calls with a non existing account
		}
		// [åˆ é™¤æ—§äº¤æ˜“]---------------------------

		// Drop all transactions that are deemed too old (low nonce)
		// åœ¨æ–°åŒºå—æ¥åˆ°åï¼Œè´¦æˆ·çš„ nonceå’Œè´¦æˆ·ä½™é¢å¦‚æœå˜åŠ¨, åˆ é™¤æ‰€æœ‰ä½äºæ–°nonceçš„äº¤æ˜“
		forwards := list.Forward(pool.currentState.GetNonce(addr))
		for _, tx := range forwards {
			hash := tx.Hash()
			pool.all.Remove(hash)
		}
		log.Trace("Removed old queued transactions", "count", len(forwards))
		// Drop all transactions that are too costly (low balance or out of gas)
		// æ ¹æ®è´¦æˆ·å¯ç”¨ä½™é¢ï¼Œæ¥ç§»é™¤äº¤æ˜“å¼€é”€ï¼ˆamount+gasLimit*gasPriceï¼‰é«˜äºæ­¤ä½™é¢çš„äº¤æ˜“
		drops, _ := list.Filter(pool.currentState.GetBalance(addr), pool.currentMaxGas)
		for _, tx := range drops {
			hash := tx.Hash()
			pool.all.Remove(hash)
		}
		log.Trace("Removed unpayable queued transactions", "count", len(drops))
		queuedNofundsMeter.Mark(int64(len(drops)))

		// [è½¬ç§»äº¤æ˜“æˆ–é‡Šæ”¾]---------------------------
		// Gather all executable transactions and promote them
		// å¦‚æœqueueé˜Ÿåˆ—ä¸­å­˜åœ¨ä½äº pending é˜Ÿåˆ—çš„æœ€å°nonceçš„äº¤æ˜“, åˆ™å¯ç›´æ¥è½¬ç§»åˆ°pendingä¸­
		readies := list.Ready(pool.pendingNonces.get(addr))
		for _, tx := range readies {
			hash := tx.Hash()
			if pool.promoteTx(addr, hash, tx) { // æå‡äº¤æ˜“ä¼˜å…ˆçº§: queue->pending
				promoted = append(promoted, tx)
			}
		}
		log.Trace("Promoted queued transactions", "count", len(promoted))
		queuedGauge.Dec(int64(len(readies)))

		// Drop all transactions over the allowed limit
		// è½¬ç§»åï¼Œè¯¥è´¦æˆ·çš„äº¤æ˜“å¯èƒ½è¶…è¿‡æ‰€å…è®¸çš„æ’é˜Ÿäº¤æ˜“ç¬”æ•°ï¼Œå¦‚æœè¶…è¿‡åˆ™ç›´æ¥ç§»é™¤è¶…è¿‡ä¸Šé™éƒ¨åˆ†çš„äº¤æ˜“ã€‚å½“ç„¶è¿™ä»…ä»…é’ˆå¯¹remoteäº¤æ˜“ã€‚
		var caps types.Transactions
		if !pool.locals.contains(addr) {
			caps = list.Cap(int(pool.config.AccountQueue))
			for _, tx := range caps {
				hash := tx.Hash()
				pool.all.Remove(hash)
				log.Trace("Removed cap-exceeding queued transaction", "hash", hash)
			}
			queuedRateLimitMeter.Mark(int64(len(caps)))
		}
		// Mark all the items dropped as removed
		pool.priced.Removed(len(forwards) + len(drops) + len(caps))
		queuedGauge.Dec(int64(len(forwards) + len(drops) + len(caps)))
		if pool.locals.contains(addr) {
			localGauge.Dec(int64(len(forwards) + len(drops) + len(caps)))
		}
		// Delete the entire queue entry if it became empty.
		if list.Empty() {
			delete(pool.queue, addr)
			delete(pool.beats, addr)
		}
	}
	return promoted
}

// truncatePending removes transactions from the pending queue if the pool is above the
// pending limit. The algorithm tries to reduce transaction counts by an approximately
// equal number for all for accounts with many pending transactions.
// æ£€æŸ¥pending äº¤æ˜“æ•°é‡
func (pool *TxPool) truncatePending() {
	pending := uint64(0)
	for _, list := range pool.pending {
		pending += uint64(len(list.txs.items))
	}
	if pending <= pool.config.GlobalSlots {
		return
	}

	pendingBeforeCap := pending
	// Assemble a spam order to penalize large transactors first
	spammers := prque.New(nil)
	for addr, list := range pool.pending {
		// Only evict transactions from high rollers
		// ä¼˜å…ˆä»è¶…ä¸Šé™çš„è´¦æˆ·ä¸­ç§»é™¤äº¤æ˜“
		if !pool.locals.contains(addr) && uint64(len(list.txs.items)) > pool.config.AccountSlots {
			spammers.Push(addr, int64(len(list.txs.items)))
		}
	}
	// Gradually drop transactions from offenders
	offenders := []common.Address{}
	for pending > pool.config.GlobalSlots && !spammers.Empty() {
		// Retrieve the next offender if not local address
		offender, _ := spammers.Pop()
		offenders = append(offenders, offender.(common.Address))

		// Equalize balances until all the same or below threshold
		if len(offenders) > 1 {
			// Calculate the equalization threshold for all current offenders
			// å­˜åœ¨ä¸€ä¸ªç‰¹æ®Šåˆ é™¤ç­–ç•¥ï¼Œå¹¶éç›´æ¥è½®æµæ¯ä¸ªè´¦æˆ·ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ªåŠ¨æ€é˜€å€¼æ§åˆ¶ï¼Œé˜€å€¼æ§åˆ¶éå†é¡ºåºï¼Œå­˜åœ¨ä¸€å®šçš„éšæœºæ€§
			threshold := len(pool.pending[offender.(common.Address)].txs.items)

			// Iteratively reduce all offenders until below limit or threshold reached
			// åœ¨ç§»é™¤äº¤æ˜“æ—¶ï¼Œå¹¶éå°†æŸä¸ªè´¦æˆ·çš„äº¤æ˜“å…¨éƒ¨åˆ é™¤ï¼Œè€Œæ˜¯æ¯ä¸ªè´¦æˆ·è½®æµåˆ é™¤ä¸€ç¬”äº¤æ˜“ï¼Œç›´åˆ°ä½äºäº¤æ˜“ä¸Šé™
			for pending > pool.config.GlobalSlots && len(pool.pending[offenders[len(offenders)-2]].txs.items) > threshold {
				for i := 0; i < len(offenders)-1; i++ {
					list := pool.pending[offenders[i]]

					caps := list.Cap(len(list.txs.items) - 1)
					for _, tx := range caps {
						// Drop the transaction from the global pools too
						hash := tx.Hash()
						pool.all.Remove(hash) // åˆ é™¤äº¤æ˜“

						// Update the account nonce to the dropped transaction
						pool.pendingNonces.setIfLower(offenders[i], tx.Nonce())
						log.Trace("Removed fairness-exceeding pending transaction", "hash", hash)
					}
					pool.priced.Removed(len(caps))
					pendingGauge.Dec(int64(len(caps)))
					if pool.locals.contains(offenders[i]) {
						localGauge.Dec(int64(len(caps)))
					}
					pending--
				}
			}
		}
	}

	// If still above threshold, reduce to limit or min allowance
	// å¦‚æœä»ç„¶è¿˜è¶…é™ï¼Œåˆ™ç»§ç»­é‡‡ç”¨ç›´æ¥éå†æ–¹å¼ï¼Œåˆ é™¤äº¤æ˜“ï¼Œç›´åˆ°ä½äºé™åˆ¶
	if pending > pool.config.GlobalSlots && len(offenders) > 0 {
		for pending > pool.config.GlobalSlots && uint64(len(pool.pending[offenders[len(offenders)-1]].txs.items)) > pool.config.AccountSlots {
			for _, addr := range offenders {
				list := pool.pending[addr]

				caps := list.Cap(len(list.txs.items) - 1)
				for _, tx := range caps {
					// Drop the transaction from the global pools too
					hash := tx.Hash()
					pool.all.Remove(hash)

					// Update the account nonce to the dropped transaction
					pool.pendingNonces.setIfLower(addr, tx.Nonce())
					log.Trace("Removed fairness-exceeding pending transaction", "hash", hash)
				}
				pool.priced.Removed(len(caps))
				pendingGauge.Dec(int64(len(caps)))
				if pool.locals.contains(addr) {
					localGauge.Dec(int64(len(caps)))
				}
				pending--
			}
		}
	}
	pendingRateLimitMeter.Mark(int64(pendingBeforeCap - pending))
}

// truncateQueue drops the oldes transactions in the queue if the pool is above the global queue limit.
// æ£€æŸ¥ queue äº¤æ˜“æ•°é‡
// äº¤æ˜“æ± å¯¹äºéå¯æ‰§è¡Œäº¤æ˜“æ•°é‡ä¹Ÿå­˜åœ¨ä¸Šé™æ§åˆ¶ã€‚å¦‚æœè¶…è¿‡ä¸Šé™ï¼ŒåŒæ ·éœ€è¦åˆ é™¤è¶…é™éƒ¨åˆ†ã€‚
func (pool *TxPool) truncateQueue() {
	queued := uint64(0)
	for _, list := range pool.queue {
		queued += uint64(len(list.txs.items))
	}
	if queued <= pool.config.GlobalQueue {
		return
	}

	// Sort all accounts with queued transactions by heartbeat
	addresses := make(addressesByHeartbeat, 0, len(pool.queue))
	for addr := range pool.queue {
		if !pool.locals.contains(addr) { // don't drop locals
			// åœ¨äº¤æ˜“è¿›å…¥pending æ—¶ä¼šæ›´æ–°è´¦æˆ·çº§çš„å¿ƒè·³æ—¶é—´ï¼Œä»£è¡¨è´¦æˆ·æœ€åpendingäº¤æ˜“æ´»åŠ¨æ—¶é—´
			addresses = append(addresses, addressByHeartbeat{addr, pool.beats[addr]})
		}
	}
	// å½“äº¤æ˜“æ± çš„äº¤æ˜“è¿‡å¤šæ—¶ï¼Œä»¥å¤ªåŠé¦–å…ˆæ ¹æ®è´¦æˆ·æ´»åŠ¨æ—¶é—´ï¼Œä»æ—©åˆ°æ™šæ’åˆ—
	sort.Sort(sort.Reverse(addresses))

	// Drop transactions until the total is below the limit or only locals remain
	for drop := queued - pool.config.GlobalQueue; drop > 0 && len(addresses) > 0; {
		addr := addresses[len(addresses)-1]
		list := pool.queue[addr.address]

		addresses = addresses[:len(addresses)-1]

		// Drop all transactions if they are less than the overflow
		// åˆ é™¤æ—¶ï¼Œå¦‚æœqueueäº¤æ˜“ç¬”æ•°ä¸å¤Ÿå¾…åˆ é™¤é‡æ—¶ï¼Œç›´æ¥æ¸…ç†è¯¥è´¦æˆ·æ‰€æœ‰queueäº¤æ˜“
		if size := uint64(len(list.txs.items)); size <= drop {
			for _, tx := range list.Flatten() {
				pool.removeTx(tx.Hash(), true)
			}
			drop -= size
			queuedRateLimitMeter.Mark(int64(size))
			continue
		}
		// Otherwise drop only last few transactions
		// å¦åˆ™é€ä¸ªåˆ é™¤ï¼Œç›´åˆ°åˆ°è¾¾åˆ é™¤ä»»åŠ¡
		txs := list.Flatten()
		for i := len(txs) - 1; i >= 0 && drop > 0; i-- {
			pool.removeTx(txs[i].Hash(), true)
			drop--
			queuedRateLimitMeter.Mark(1)
		}
	}
}

// demoteUnexecutables removes invalid and processed transactions from the pools
// executable/pending queue and any subsequent transactions that become unexecutable
// are moved back into the future queue.
//
// Note: transactions are not marked as removed in the priced list because re-heaping
// is always explicitly triggered by SetBaseFee and it would be unnecessary and wasteful
// to trigger a re-heap is this function
func (pool *TxPool) demoteUnexecutables() {
	// Iterate over all accounts and demote any non-executable transactions
	for addr, list := range pool.pending {
		nonce := pool.currentState.GetNonce(addr)

		// Drop all transactions that are deemed too old (low nonce)
		olds := list.Forward(nonce)
		for _, tx := range olds {
			hash := tx.Hash()
			pool.all.Remove(hash)
			log.Trace("Removed old pending transaction", "hash", hash)
		}
		// Drop all transactions that are too costly (low balance or out of gas), and queue any invalids back for later
		drops, invalids := list.Filter(pool.currentState.GetBalance(addr), pool.currentMaxGas)
		for _, tx := range drops {
			hash := tx.Hash()
			log.Trace("Removed unpayable pending transaction", "hash", hash)
			pool.all.Remove(hash)
		}
		pendingNofundsMeter.Mark(int64(len(drops)))

		for _, tx := range invalids {
			hash := tx.Hash()
			log.Trace("Demoting pending transaction", "hash", hash)

			// Internal shuffle shouldn't touch the lookup set.
			pool.enqueueTx(hash, tx, false, false)
		}
		pendingGauge.Dec(int64(len(olds) + len(drops) + len(invalids)))
		if pool.locals.contains(addr) {
			localGauge.Dec(int64(len(olds) + len(drops) + len(invalids)))
		}
		// If there's a gap in front, alert (should never happen) and postpone all transactions
		if len(list.txs.items) > 0 && list.txs.Get(nonce) == nil {
			gapped := list.Cap(0)
			for _, tx := range gapped {
				hash := tx.Hash()
				log.Error("Demoting invalidated transaction", "hash", hash)

				// Internal shuffle shouldn't touch the lookup set.
				pool.enqueueTx(hash, tx, false, false)
			}
			pendingGauge.Dec(int64(len(gapped)))
			// This might happen in a reorg, so log it to the metering
			blockReorgInvalidatedTx.Mark(int64(len(gapped)))
		}
		// Delete the entire pending entry if it became empty.
		if list.Empty() {
			delete(pool.pending, addr)
		}
	}
}

// addressByHeartbeat is an account address tagged with its last activity timestamp.
type addressByHeartbeat struct {
	address   common.Address
	heartbeat time.Time
}

type addressesByHeartbeat []addressByHeartbeat

func (a addressesByHeartbeat) Len() int           { return len(a) }
func (a addressesByHeartbeat) Less(i, j int) bool { return a[i].heartbeat.Before(a[j].heartbeat) }
func (a addressesByHeartbeat) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }

// accountSet is simply a set of addresses to check for existence, and a signer
// capable of deriving addresses from transactions.
type accountSet struct {
	accounts map[common.Address]struct{}
	signer   types.Signer
	cache    *[]common.Address
}

// newAccountSet creates a new address set with an associated signer for sender
// derivations.
func newAccountSet(signer types.Signer, addrs ...common.Address) *accountSet {
	as := &accountSet{
		accounts: make(map[common.Address]struct{}),
		signer:   signer,
	}
	for _, addr := range addrs {
		as.add(addr)
	}
	return as
}

// contains checks if a given address is contained within the set.
func (as *accountSet) contains(addr common.Address) bool {
	_, exist := as.accounts[addr]
	return exist
}

func (as *accountSet) empty() bool {
	return len(as.accounts) == 0
}

// containsTx checks if the sender of a given tx is within the set. If the sender
// cannot be derived, this method returns false.
func (as *accountSet) containsTx(tx *types.Transaction) bool {
	if addr, err := types.Sender(as.signer, tx); err == nil {
		return as.contains(addr)
	}
	return false
}

// add inserts a new address into the set to track.
func (as *accountSet) add(addr common.Address) {
	as.accounts[addr] = struct{}{}
	as.cache = nil
}

// addTx adds the sender of tx into the set.
func (as *accountSet) addTx(tx *types.Transaction) {
	if addr, err := types.Sender(as.signer, tx); err == nil {
		as.add(addr)
	}
}

// flatten returns the list of addresses within this set, also caching it for later
// reuse. The returned slice should not be changed!
func (as *accountSet) flatten() []common.Address {
	if as.cache == nil {
		accounts := make([]common.Address, 0, len(as.accounts))
		for account := range as.accounts {
			accounts = append(accounts, account)
		}
		as.cache = &accounts
	}
	return *as.cache
}

// merge adds all addresses from the 'other' set into 'as'.
func (as *accountSet) merge(other *accountSet) {
	for addr := range other.accounts {
		as.accounts[addr] = struct{}{}
	}
	as.cache = nil
}

// txLookup is used internally by TxPool to track transactions while allowing
// lookup without mutex contention.
//
// Note, although this type is properly protected against concurrent access, it
// is **not** a type that should ever be mutated or even exposed outside of the
// transaction pool, since its internal state is tightly coupled with the pools
// internal mechanisms. The sole purpose of the type is to permit out-of-bound
// peeking into the pool in TxPool.Get without having to acquire the widely scoped
// TxPool.mu mutex.
//
// This lookup set combines the notion of "local transactions", which is useful
// to build upper-level structure.
// é‡‡ç”¨ä¸€ä¸ª txLookup (å†…éƒ¨ä¸ºmapï¼‰è·Ÿè¸ªæ‰€æœ‰äº¤æ˜“
type txLookup struct {
	slots   int
	lock    sync.RWMutex
	locals  map[common.Hash]*types.Transaction // æœ¬åœ°äº¤æ˜“
	remotes map[common.Hash]*types.Transaction // è¿œç¨‹äº¤æ˜“
}

// newTxLookup returns a new txLookup structure.
func newTxLookup() *txLookup {
	return &txLookup{
		locals:  make(map[common.Hash]*types.Transaction),
		remotes: make(map[common.Hash]*types.Transaction),
	}
}

// Range calls f on each key and value present in the map. The callback passed
// should return the indicator whether the iteration needs to be continued.
// Callers need to specify which set (or both) to be iterated.
func (t *txLookup) Range(f func(hash common.Hash, tx *types.Transaction, local bool) bool, local bool, remote bool) {
	t.lock.RLock()
	defer t.lock.RUnlock()

	if local {
		for key, value := range t.locals {
			if !f(key, value, true) {
				return
			}
		}
	}
	if remote {
		for key, value := range t.remotes {
			if !f(key, value, false) {
				return
			}
		}
	}
}

// Get returns a transaction if it exists in the lookup, or nil if not found.
func (t *txLookup) Get(hash common.Hash) *types.Transaction {
	t.lock.RLock()
	defer t.lock.RUnlock()

	if tx := t.locals[hash]; tx != nil {
		return tx
	}
	return t.remotes[hash]
}

// GetLocal returns a transaction if it exists in the lookup, or nil if not found.
func (t *txLookup) GetLocal(hash common.Hash) *types.Transaction {
	t.lock.RLock()
	defer t.lock.RUnlock()

	return t.locals[hash]
}

// GetRemote returns a transaction if it exists in the lookup, or nil if not found.
func (t *txLookup) GetRemote(hash common.Hash) *types.Transaction {
	t.lock.RLock()
	defer t.lock.RUnlock()

	return t.remotes[hash]
}

// Count returns the current number of transactions in the lookup.
func (t *txLookup) Count() int {
	t.lock.RLock()
	defer t.lock.RUnlock()

	return len(t.locals) + len(t.remotes)
}

// LocalCount returns the current number of local transactions in the lookup.
func (t *txLookup) LocalCount() int {
	t.lock.RLock()
	defer t.lock.RUnlock()

	return len(t.locals)
}

// RemoteCount returns the current number of remote transactions in the lookup.
func (t *txLookup) RemoteCount() int {
	t.lock.RLock()
	defer t.lock.RUnlock()

	return len(t.remotes)
}

// Slots returns the current number of slots used in the lookup.
func (t *txLookup) Slots() int {
	t.lock.RLock()
	defer t.lock.RUnlock()

	return t.slots
}

// Add adds a transaction to the lookup.
func (t *txLookup) Add(tx *types.Transaction, local bool) {
	t.lock.Lock()
	defer t.lock.Unlock()

	t.slots += numSlots(tx)
	slotsGauge.Update(int64(t.slots))

	if local {
		t.locals[tx.Hash()] = tx
	} else {
		t.remotes[tx.Hash()] = tx
	}
}

// Remove removes a transaction from the lookup.
func (t *txLookup) Remove(hash common.Hash) {
	t.lock.Lock()
	defer t.lock.Unlock()

	tx, ok := t.locals[hash]
	if !ok {
		tx, ok = t.remotes[hash]
	}
	if !ok {
		log.Error("No transaction found to be deleted", "hash", hash)
		return
	}
	t.slots -= numSlots(tx)
	slotsGauge.Update(int64(t.slots))

	delete(t.locals, hash)
	delete(t.remotes, hash)
}

// RemoteToLocals migrates the transactions belongs to the given locals to locals
// set. The assumption is held the locals set is thread-safe to be used.
func (t *txLookup) RemoteToLocals(locals *accountSet) int {
	t.lock.Lock()
	defer t.lock.Unlock()

	var migrated int
	for hash, tx := range t.remotes {
		if locals.containsTx(tx) {
			t.locals[hash] = tx
			delete(t.remotes, hash)
			migrated += 1
		}
	}
	return migrated
}

// RemotesBelowTip finds all remote transactions below the given tip threshold.
func (t *txLookup) RemotesBelowTip(threshold *big.Int) types.Transactions {
	found := make(types.Transactions, 0, 128)
	t.Range(func(hash common.Hash, tx *types.Transaction, local bool) bool {
		if tx.GasTipCapIntCmp(threshold) < 0 {
			found = append(found, tx)
		}
		return true
	}, false, true) // Only iterate remotes
	return found
}

// numSlots calculates the number of slots needed for a single transaction.
func numSlots(tx *types.Transaction) int {
	return int((tx.Size() + txSlotSize - 1) / txSlotSize)
}
